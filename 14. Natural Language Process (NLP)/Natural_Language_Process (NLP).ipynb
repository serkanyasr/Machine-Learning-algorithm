{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# import data\n","data = pd.read_csv(r\"data.csv\",encoding=\"latin1\")\n","data = pd.concat([data.gender,data.description],axis=1)\n","data.dropna(axis=0,inplace=True)\n","# Kadın=1, erkenk =0\n","data.gender = [1 if i==\"female\" else 0 for i in data.gender]\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# data cleaning regular expressiosn\n","import re\n","first_desc = data.description[4]\n","desc = re.sub(\"[^a-zA-Z]\",\" \",first_desc)   #A-Z ye kadar olan kelimeleri tara eşit değilse onun yerine boşluk koy\n","# ^ bu işaret ile a-z,A_Z olanları bulma diğerlerini boşluk ile değiştir\n","desc = desc.lower() # tüm harfleri küçük harflerie çevirme\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\yasar\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\yasar\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\yasar\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["#stopwords(alakasız kelimeler) temizleme yani ve veya ki de da the gibi kelimler\n","import nltk  \n","nltk.download(\"stopwords\")\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords\n","# desc = desc.split() # tüm kelimeleri boşluklardan ayırır ama don't ile olanları ayırıamaz\n","desc = nltk.word_tokenize(desc)\n","# gerekszi kelimelerei çıkar\n","desc = [ i for i in desc if not i in set(stopwords.words(\"english\"))]\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["['ricky',\n"," 'wilson',\n"," 'best',\n"," 'frontman',\n"," 'kaiser',\n"," 'chief',\n"," 'best',\n"," 'band',\n"," 'xxxx',\n"," 'thank',\n"," 'kaiser',\n"," 'chief',\n"," 'incredible',\n"," 'year',\n"," 'gig',\n"," 'memory',\n"," 'cherish',\n"," 'always',\n"," 'xxxxxxx']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# kelimelerde çekimleri yani kelimien köklerini bulmalıyız\n","\n","import nltk as nlp\n","\n","lemma = nlp.WordNetLemmatizer()\n","desc = [lemma.lemmatize(i) for i in desc]\n","desc\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'ricky wilson best frontman kaiser chief best band xxxx thank kaiser chief incredible year gig memory cherish always xxxxxxx'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#\n","desc = \" \".join(desc)\n","desc"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# tüm data için uygulama\n","desc_list=[]\n","for description in data.description:\n","    desc = re.sub(\"[^a-zA-Z]\",\" \",description)   #A-Z ye kadar olan kelimeleri tara eşit değilse onun yerine boşluk koy\n","    desc = desc.lower() # tüm harfleri küçük harflerie çevirme\n","    desc = nltk.word_tokenize(desc)\n","    desc = [ i for i in desc if not i in set(stopwords.words(\"english\"))]\n","    desc = [lemma.lemmatize(i) for i in desc]\n","    desc = \" \".join(desc)\n","    desc_list.append(desc)\n","    \n","    "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# bag of words kelimelerin çantası \n","from sklearn.feature_extraction.text import CountVectorizer\n","max_feature =500 # en çok kullanılan kelimlerin 500 seç\n","count_vectorizer = CountVectorizer(max_features=max_feature,stop_words=\"english\")\n","\n","sparce_matrix = count_vectorizer.fit_transform(desc_list).toarray()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# print(\"En sık Kullanılan {} kelime: {}\".format(max_feature,count_vectorizer.get_feature_names()))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["array([0, 0, 0, ..., 0, 1, 1], dtype=int64)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["data.gender.values"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["y =data.gender.values # erkek mi kız mı1\n","x=sparce_matrix\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.1,random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","nb = GaussianNB()\n","nb.fit(x_train,y_train)\n","y_pred = nb.predict(x_test)\n","\n","nb.score(y_pred.reshape(-1,1),y_test)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNuka2N8YfQTGtfSPIWNoXK","collapsed_sections":[],"mount_file_id":"1USEe4C5dcfaFfPFuzyIichMfM5s1IsKf","name":"CNN_Sample1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"9753a285cac4fec0c2f3818cc046ef4b040aeac25440251d812c5d25e37bebd9"}}},"nbformat":4,"nbformat_minor":0}
